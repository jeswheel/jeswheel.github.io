[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "USU.html",
    "href": "USU.html",
    "title": "Stats 5100: Model Selection",
    "section": "",
    "text": "For today’s lecture I will use the R package learnr. If you would like to follow along interactively, you can download this file from my website: jeswheel.github.io.\n\nlibrary(learnr)     # For interactive exercises\n\nIf you “run” the document in R-Studio, it will download the learnr package for you."
  },
  {
    "objectID": "USU.html#set-up",
    "href": "USU.html#set-up",
    "title": "Stats 5100: Model Selection",
    "section": "",
    "text": "For today’s lecture I will use the R package learnr. If you would like to follow along interactively, you can download this file from my website: jeswheel.github.io.\n\nlibrary(learnr)     # For interactive exercises\n\nIf you “run” the document in R-Studio, it will download the learnr package for you."
  },
  {
    "objectID": "USU.html#logistics",
    "href": "USU.html#logistics",
    "title": "Stats 5100: Model Selection",
    "section": "Logistics",
    "text": "Logistics\n\nAny questions about previous material?"
  },
  {
    "objectID": "USU.html#introduction",
    "href": "USU.html#introduction",
    "title": "Stats 5100: Model Selection",
    "section": "Introduction",
    "text": "Introduction\n\nReview\nWe are interested in fitting a linear regression model of the form:\n\\[\ny_i = \\beta_0 + \\beta_1x_{1, i} + \\ldots + \\beta_{p-1}x_{p-1, i} + \\beta_px_{p, i} + \\epsilon_i,\n\\] Where \\(\\epsilon_i\\) for all \\(i = 1, 2, \\ldots, n\\) are i.i.d. random noise terms with mean zero \\(\\big(\\text{E}[\\epsilon_i] = 0\\big)\\) and variance \\(\\sigma^2\\) \\(\\big(\\text{Var}(\\epsilon_i) = \\sigma^2\\big)\\).\n\nIn this current setup, there are \\(n\\) observations and \\(p\\) variables.\n\n\n\nModel Selection\nIn real data analysis situations, people often have data for a response variable (\\(\\mathbf{y}\\)) and many possible regressors (\\(\\{\\mathbf{x_i}\\}_{i = 1}^p\\)).\nInstead of fitting a model that describes the response variable using every available regressor, we are often interested in fitting a model to only a subset of the variables \\(\\{\\mathbf{x_i}\\}_{i = 1}^p\\).\nFor example, suppose that \\(p = 20\\) and we only want to include variables \\(\\mathbf{x}_{2}, \\mathbf{x}_{10}\\), and \\(\\mathbf{x}_{13}\\):\n\\[\ny_i = \\beta_0 + \\beta_2x_{2, i} + \\beta_{10}x_{10, i} + \\beta_{13}x_{13, i} + \\epsilon_i,\n\\]\n\n\nQuestion\nWhat are some reasons that we might want to fit a model to only a subset of predictors?\n\n\nSome Possible reasons:\n\nOverfitting\n\nWhen a model includes too many predictors, it can capture noise rather than the underlying relationship in the data.\n\nMulticollinearity\n\nIncluding highly correlated predictors can make it difficult to discern the individual effect of each predictor.\n\nInterpretability\n\nModels with many predictors can be difficult to describe to stakeholders; models with fewer variables can be more easily understood (and trusted) by non-statistical audiences.\nThis is a recurring theme in statistics, sometimes called parsimony: the simplest model that adequately describes the data is preferred (Occam’s Razor).\n\nCost and Feasibility\n\nThe model may be used to make predictions about newly obtained data, or motivate the collection of new data for future studies. If it is expensive / inconvenient to collect variables, it’s good to know which subset might be most important to collect.\n\n\n\n\nWhat we will cover today\nToday I hope to show give some examples demonstrating why we might want to do model selection, and common techniques used to select the subset model.\nThere will be some interactive exercises, so if you would like to participate and you’re having any trouble accessing this document, now is the time to let me know."
  },
  {
    "objectID": "USU.html#overfitting",
    "href": "USU.html#overfitting",
    "title": "Stats 5100: Model Selection",
    "section": "Overfitting",
    "text": "Overfitting\n\nStatistical Bias\nSuppose we are interested in an estimate of \\(\\mathbf{\\beta}\\), which we denote \\(\\hat{\\mathbf{\\beta}}\\).\nWe would hope that the expected value of our estimate would be equal to the thing we are estimating:\n\\[\n\\text{E}[\\hat{\\mathbf{\\beta}}] = \\beta \\hspace{4mm} \\text{ or equivalently,} \\hspace{4mm} 0 = \\text{E}[\\hat{\\mathbf{\\beta}}] - \\beta\n\\] The difference \\(b = \\text{E}[\\hat{\\mathbf{\\beta}}] - \\beta\\) is known as the bias of an estimate.\n\nThe bias can be thought of as an consistent error (for example, under or over estimation), and is often the result of having a model that is too simple.\n\n\n\nVariance of an estimator\nUsing this same notation, the variance of the estimator is defined as: \\(\\text{Var}(\\hat{\\mathbf{\\beta}})\\).\n\nIn this context, we think of this variance as how much our parameter estimates change if there are small changes in the training data. Large variance is often a result of having a model that is too complex.\n\n\n\nBias-Variance tradeoff\n\nAs the number of parameters of a model increase (i.e., more model complexity), the model can better fit the data, reducing bias.\nHowever, larger models have increased variance, as every time we obtain a different sample the model fit will change.\n\nFor regression models, it can be shown that the total expected error is the sum of the variance of your estimator and the squared bias:\n\\[\n\\text{Expected error} =  \\text{Var}(\\hat{\\mathbf{\\beta}}) + \\big(\\text{E}[\\hat{\\mathbf{\\beta}}] - \\beta\\big)^2\n\\] Because our goal is to minimize the total error, we want to find a model that balances this bias-variance tradeoff: Bias-variance figure 1, Bias-variance figure 2.\n\n\nExample\nSuppose that I have a response variable \\(y\\) and only a single predictor variable \\(x\\) that have a non-linear relationship:\n\\[\ny_i = f(x_i) + \\epsilon_i\n\\]\nI have simulated such as example in R, saved into an object called df_poly.\n\n\n\n\n\n\n\n\n\nIf I don’t know what this relationship is, I might consider fitting a polynomial regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2 + \\ldots + \\beta_px_p^p + \\epsilon_p\n\\] The question remains: how many variables should I fit to the data?\n\n\n\n\n\n\n\n\n\n\n\nExercise: Fitting polynomial regression\nTry finding a value of \\(p\\) that provides a suitable fit to the data in df_poly.\nTo fit a polynomial regression model in R, we need to use the following for our formulas:\nlm(Y~X + I(X^2) + ... + I(X^p), data = df_poly)\nHere the function \\(I()\\) tells R to interpret X^2 as the square of X.\n\n\nhead(df_poly)\n\n           x        y\n1  0.5855288 13.78895\n2  0.7094660 14.55087\n3 -0.1093033 15.12250\n4 -0.4534972 15.41938\n5  0.6058875 16.05119\n6 -1.8179560 12.16711\n\n\n\n\n\n\n# You can fit a 2-degree polynomial regression model using:\npoly_fit &lt;- lm(Y~x+I(x^2), data = df_poly)\n\n# To get a diagnostic summary, try plotting the output:\nplot(poly_fit)\n\n# Or take a summary:\nsummary(poly_fit)\n\n\n\n\n# You can easily plot your fit using ggplot: \nggplot(data = df_poly, aes(x = x, y = y)) + \n  geom_point() + \n  geom_smooth(\n    method = 'lm', \n    formula = y ~ x + I(x^2)\n  )\n\n# In this case, I'm plotting a 2-degree polynomial. \n# The only thing you need to change is to plug in the formula that you would like to plot. \n\n\n\n\n\n\n\n\nQuestion:\nPreviously, we have discussed methods of assessing the goodness-of-fit of any given regression model (for example, \\(\\text{R}^2\\), MSE and visual checks of residuals).\nProblem: if we use these evaluations on the data we use to fit the model, they will always suggest using a larger model.\nQuestion: What do you think we might try in order to perform model selection?\n\n\nModel Selection Methods\n\nA statistical test of significance (ANOVA).\nConsider adding a penalty to model diagnostics."
  },
  {
    "objectID": "USU.html#statistical-testing",
    "href": "USU.html#statistical-testing",
    "title": "Stats 5100: Model Selection",
    "section": "Statistical Testing",
    "text": "Statistical Testing\nWe can actually perform a statistical test comparing a larger model to a nested version of the model.\nFor instance, consider the base model with four variables (which we call \\(M_1\\)):\n\\[\ny_i = \\beta_0 + \\beta_1x_{1, i} + \\beta_2x_{2, i} + \\beta_3x_{3, i} + \\beta_4x_{4, i} + \\epsilon_i,\n\\] and we would like to compare this to a smaller nested model (which we call \\(M_0\\)): \\[\ny_i = \\beta_0 + \\beta_2x_{2, i} + \\beta_3x_{3, i} + \\epsilon_i,\n\\]\nThis smaller model is considered nested because it’s actually the same model, just setting \\(\\beta_1 = \\beta_4 = 0\\).\n(Here I have chosen variables and coefficients corresponding to the indices \\(2, 3\\) as an example, but this works for arbitrary numbers of original variables \\(p\\) and subset sizes).\n\nWe can calculate an F-test statistic by considering the ratio: \\[\nF = \\frac{\\Delta\\text{SSE}/\\Delta p}{\\text{MSE}_1},\n\\] where \\(\\Delta\\text{SSE} = \\text{SSE}_0 - \\text{SSE}_1\\), and \\(\\Delta p\\) is the difference in the number of model parameters (i.e., the number of coefficients being tested).\n\n\nAdvantages\n\nSimplicity\nFormal Hypothesis testing framework\nEasy to implement\n\n\n\nDisadvantages\n\nCan be too conservative\n\nWe only pick the larger model if we reject the null hypothesis. The test is designed to control type I errors, meaning we want to limit number of times we reject if the null is true (leading to fewer rejections and selecting simple models).\n\nIssues with multiple-testing\n\nIf we want to do this test for multiple nested models, we run into a multiple-testing issue, which means we may not be controlling the type-I error rate.\n\nAssumes normally distributed error terms.\n\n\n\nImplementation in R\nIn R, there are various ways you can do this type of test, but an easy way is using the anova function. This is demonstrated below on the mtcars dataset, which contains data about cars in the 1970’s:\n\n\nsummary(mtcars)\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) + \n  geom_point() + \n  labs(x = \"Car Weight (1000 lbs)\", y = \"Miles/Gallon\")\n\n\n\nThere seems to be a strong correlation between the weight of a car (wt) and its gas mileage (mpg). We can test this model against and alternative that includes additional variables:\n\n\n# Fit the full model\nfull_model &lt;- lm(mpg ~ wt + hp + qsec + disp + drat, data = mtcars)\n\n# Fit the reduced model (nested within the full model)\nreduced_model &lt;- lm(mpg ~ wt, data = mtcars)\n\nanova(reduced_model, full_model)"
  },
  {
    "objectID": "USU.html#adjusting-model-diagnostics",
    "href": "USU.html#adjusting-model-diagnostics",
    "title": "Stats 5100: Model Selection",
    "section": "Adjusting model diagnostics",
    "text": "Adjusting model diagnostics\nAnother approach is to adjust model diagnostics by adding a penalty for larger models.\n\nAdjusted \\(R^2\\)\nOne example is to penalize the \\(\\text{R}^2\\) statistic. Recall that \\(\\text{R}^2\\) represents the proportion of variation in the data that can be explained by the model.\n\n\\(\\text{R}^2\\) can be written as: \\[\nR^2 = 1 - \\frac{\\text{SS}_{\\text{error}}}{\\text{SS}_{\\text{total}}}\n\\]\nThe adjusted \\(\\text{R}^2\\) is: \\[\nR_{\\text{adj}}^2 = 1 - \\frac{\\text{SS}_{\\text{error}}/(n-p)}{\\text{SS}_{\\text{total}}/(n-1)} = 1 - \\frac{(1 - \\text{R}^2)(n - 1)}{n - p - 1}\n\\]\n\nWhile \\(\\text{R}^2\\) always increases when extra variables are added to the model, adjusted \\(\\text{R}^2\\) only increases when the improvement in \\(\\text{R}^2\\) (due to the inclusion of a new variable) is more than one would expect to see by chance.\nIn R, the adjusted \\(\\text{R}^2\\) statistic can be obtained by summarizing an lm object:\n\n\ncar_mod &lt;- lm(mpg~cyl+disp+hp+wt, data = mtcars)\nsummary(car_mod)\n\n\n\n\n\nAIC\nThe log-likelihood of a model is a useful comparison tool; a model with a higher log-likelihood describes the available data better than a model with lower log-likelihood.\nLike \\(\\text{R}^2\\), the log-likelihood does not account for model complexity, and therefore adding new variables into the model will always increase the log-likelihood.\n\nAkaike’s information criterion (AIC) adjusted the log-likelihood by adding a penalty for the number of variables used. \\[\n\\text{AIC} = 2p - 2 \\times \\text{log-likelihood}\n\\]\nGiven a set of candidate models, the preferred model is the one with the smallest AIC.\n\nAIC was first developed in the case where models parameters are estimated by maximum likelihood estimation (i.e., finding parameter values that have the highest log-likelihood). If the model was fit in this way, then the AIC has some interesting theoretical properties.\nIn R, the AIC statistic can be obtained by using the AIC function on an lm object:\n\n\ncar_mod1 &lt;- lm(mpg~cyl+disp+hp+wt, data = mtcars)\ncar_mod2 &lt;- lm(mpg~hp+wt, data = mtcars)\nAIC(car_mod1)\nAIC(car_mod2)\n\n\n\n\n\nAdditional diagnostics\nThere are various other diagnostic statistics that we might use to compare models.\n\nApproaches that adjust for model complexity: adjusted \\(\\text{R}^2\\), AIC, AICC, BIC, Mallow’s \\(C_p\\).\nThere are also approaches that do not adjust for model complexity, such as the MSE.\n\nQuestion: How do we know which to use?"
  },
  {
    "objectID": "USU.html#testing-multiple-model-variations",
    "href": "USU.html#testing-multiple-model-variations",
    "title": "Stats 5100: Model Selection",
    "section": "Testing multiple model variations",
    "text": "Testing multiple model variations\n\nStepwise regression\nSo far we have only discussed approaches for comparing two candidate models. In order to find an optimal subset of variables to use in a model, we could fit all possible models using a subset of the available predictor variables.\nSuppose we have \\(p\\) predictor variables available. Then the total number of candidate models is \\(2^p\\). Even with a relatively small number \\(p = 20\\), this would require fitting more than one million models!\nTo find a suitable subset model, we will focus on stepwise regression techniques. These approaches are greedy algorithms (a greedy algorithm won’t necessarily give us the “best” answer, but will give an answer based on locally optimal solutions).\nThese algorithms include forward selection, backward elimination, and bidirectional elimination (aka forward-backward elimination).\n\n\nForward Selection\n\nStart with a model with no variables.\nEvaluate the effect of adding each remaining candidate variable (one at a time) to the current model.\nAdd the variable that most improves the model according to the chosen criterion (e.g., decreases AIC/BIC the most, increases adjusted R-squared the most, etc.).\nRepeat until no variables improve the model based on our criterion.\n\n\n\nBackwards Elimination\n\nStart with all variables.\nTest the removal of each variable using the chosen criterion, and remove a variable if the criterion improves with the removal of the variable.\nRepeat this process until no further variables can be removed without a reduction in the selected criterion.\n\n\n\nBidirectional elimination (forward-backward)\n\nCombination of both forward and backward elimination.\nStart with a candidate model (often either intercept only, or all variables).\nIf possible, do a forward step by considering to add a single variable (one at a time) to the current model.\nAfter adding a variable, evaluate the effect of removing each variable (one at a time) from the current model (if possible).\nContinue this process until no more variables can be added or removed to improve the model further.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to use AIC as the criterion, each of these approaches can be implemented in R with the step function.\nExercise: Try using the step function in R to select a model for the mtcars dataset.\n\n\nnull_model &lt;- lm(mpg ~ 1, data = mtcars)\nfull_model &lt;- lm(mpg ~ ., data = mtcars)\n\nstep(full_model, direction = 'backward')\n\n\n\n\n\n# To go forward, you need to provide a \"Scope\" \n# The scope basically is the largest model you want it to consider: \nscope = list(upper = full_model)\n\n\n\n\n# To do bidirectional, you need scope to have a lower and an upper\nscope = list(upper = full_model, lower = null_model)\n\n\n\n\n#### SOLUTION #### \n\n# Backwards\nstep(full_model, direction = 'backward')\n\n# Forwards\nstep(\n  full_model, direction = 'forward',\n  scope = list(upper = full_model)\n)\n\n# Forward-Backward\nstep(\n  null_model, direction = 'both',\n  scope = list(upper = full_model, lower = null_model)\n)\n\n# Backward-Forward \nstep(\n  full_model, direction = 'both',\n  scope = list(upper = full_model, lower = null_model)\n)"
  },
  {
    "objectID": "USU.html#cross-validation",
    "href": "USU.html#cross-validation",
    "title": "Stats 5100: Model Selection",
    "section": "Cross validation",
    "text": "Cross validation\nWhen we fit a model to data, we are typically interested in how well our fitted model generalized to new datasets.\nUnfortunately, we usually can’t directly assess how well the model generalizes because we don’t have new data.\n\nTesting data\nIn an ideal scenario, we can have two distinct datasets. We will use one for training (call it the training data), and another for testing (call it the testing data).\nTo see why this is useful, let’s revisit the polynomial regression example. We start with the same 200 observations that we had before, but now let’s assume I have a second dataset from the same data generating process that has 1000 observations (saved as df_poly_test).\nWe can now consider how many polynomial regression variables should be included in our model based on how well the model fits the test data. We will measure goodness-of-fit in this case using RMSE.\n\n\n# Function to evaluate MSE of model predictions\nMSE &lt;- function(Y, Y_hat) {\n  mean((Y - Y_hat)^2)\n}\n\n# Set the maximum degree of the polynomial model\nmax_p &lt;- 7\n\n# Create a data.frame object to save results of training data\nresults_train &lt;- data.frame(\n  degree = 1:max_p, \n  MSE = NA_real_, \n  set = 'train'\n)\n\n# Create a data.frame object to save results of testing data \nresults_test &lt;- data.frame(\n  degree = 1:max_p, \n  MSE = NA_real_, \n  set = 'test'\n)\n\n# For each p in 1:max_p, fit a polynomial model of degree p, and calculate \n# the MSE on the training and testing data.\nfor (p in 1:max_p) {\n  \n  # Fit model on training data \n  poly_model &lt;- lm(y~poly(x, p), data = df_poly)\n  \n  # Make Y predictions on training data\n  train_predictions &lt;- predict(poly_model, newdata = df_poly)\n  \n  # Make Y predictions on testing data \n  test_predictions &lt;- predict(poly_model, newdata = df_poly_test)\n  \n  # Calculate MSE and save the results \n  results_train[p, 'MSE'] &lt;- MSE(df_poly$y, train_predictions)\n  results_test[p, 'MSE'] &lt;- MSE(df_poly_test$y, test_predictions)\n}\n\n# Combine results into a single data.frame object \nresults &lt;- dplyr::bind_rows(\n  results_train, results_test\n)\n\n# Plot the results\nggplot(results, aes(x = degree, y = MSE, col = set)) + \n  geom_line(aes(y = MSE)) + \n  xlab(\"Polynomial Degree\")\n\n\n\nNote that testing works best if you have a lot a data that you can use for testing.\n\n\nData Splitting\nIn the absence of a new dataset we can test on, we have to figure something else out.\nIf we have a large amount of data, one idea is to split the data into two parts: one for training, and one for testing.\nWe don’t necessarily have to split the data evenly. For example, it’s common to see a split of 80/20, where \\(80\\%\\) of the data are used for training.\nIf there is a lot of data available, then the test set provide a representative sample of how the model performs on unseen data.\nQuestion: How is this different from just having two separate datasets? Why might the this cause a disadvantage?\nQuestion: What could we try if we don’t have a lot of data?\n\n\n\\(K\\)-fold cross-validation\nA sampling procedure that reuses a limited data sample to evaluate model predictive performance.\nIn a \\(K\\)-fold cross validation (CV):\n\nDivide the data sample (evenly) into \\(K\\) sets (folds).\nIn each fold, use (\\(K - 1\\)) sets for model fitting (i.e. training), and the remaining set for evaluating model prediction (i.e., testing).\nRepeat the above step \\(K\\) times, rotating the training and test sets.\nEvaluate the overall performance by averaging the results across folds.\n\nHere is a visual representation of \\(5\\)-fold cross validation.\n\n\nComments on CV\n\nPopular choices of \\(K\\) (number of folds) are \\(3, 5, 10, 20\\), with \\(10\\) being the most common.\nAnother choice is called leave-one-out cross validation (LOOCV), which is like picking \\(K\\) to be the number observations in the data. This can be computationally prohibitive.\n\nSome relative terminology includes:\n\nin-sample evaluation: model evaluation done on the training data.\nout-of-sample evaluation: model evaluation on data the trained model hasn’t seed (i.e., test set)."
  },
  {
    "objectID": "USU.html#interactive-exercise",
    "href": "USU.html#interactive-exercise",
    "title": "Stats 5100: Model Selection",
    "section": "Interactive Exercise",
    "text": "Interactive Exercise\nYou have just learned about K-Fold Cross Validation in our previous class. Now, you will apply this technique to the mtcars dataset in R to evaluate the performance of a linear regression model for the variable mpg.\nYou can pick any regressors you would like, but a good starting point might be the formula: mpg ~ disp + hp + wt.\nSteps:\n\nShuffle the dataset. Randomly shuffle the rows of the dataset.\nSplit the dataset into K folds. For this task, let K = 5.\nPerform K-fold cross-validation.\n\nFor each fold, fit a linear regression model on the training data (i.e., all folds except the current fold).\nEvaluate the model’s performance on the test data (i.e., the current fold).\nRecord the prediction error for each fold. Use Mean Squared Error (MSE) as the metric.\n\nCalculate the average prediction error across all K folds.\n\nImplement this procedure from scratch using R. There are multiple ways you could do this, so the provided hints demonstrate only one possible approach.\n\n\nset.seed(1)  # for reproducibility\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n# First we want to shuffle the data. Why? \n# Sometimes there may be some features of the data recording process so that \n# data that are close to each other in the dataset may be more closely related \n# than those that are further apart. We want to avoid this in our evaluations. \nset.seed(1)\ndf_cars &lt;- mtcars[sample(1:nrow(mtcars)), ]\n\n\n\n\n# Setting up the k-folds. \n\n# The number of observations cannot be evenly split into 5 folds. You need \n# to account for this somehow. \n\n# One possible approach:\n#   (7 observations in folds 1 & 2, 6 observations in all other folds)\nfolds &lt;- rep(c(1, 2, 3, 4, 5), length.out = nrow(df_cars))\n\n\n\n\n# You may need to do the following: \n\n# Set up a vector to save the results for each fold: \nmse_results &lt;- c()\n\n# Create a for-loop over the number of folds: \nfor (k in 1:5) {\n  # ... \n}\n\n# Within the for-loop, get training and testing sets for current value of k. \n# Here's how you might get the test set if k == 1: \nis_test_set &lt;- folds == 1 \ndf_test &lt;- df_cars[is_test_set, ]\n### How can you extend this for all k, and to both train and test sets? \n\n\n# You also need to fit the model, evaluate, and save results within the loop.\n# For example: \nmod_k &lt;- lm(mpg ~ disp + hp + wt, data = df_train)\npredict_k &lt;- predict(mod_k, newdata = df_test)\nmse_k &lt;- mean((predict_k - df_test$mpg)^2)\nmse_results &lt;- c(mse_results, mse_k)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jesse Wheeler",
    "section": "",
    "text": "I am a 5th year PhD student in the Statistics Department at the University of Michigan. I research likelihood based methods for partially observed Markov process (POMP) models. I primarily work on theory, methodology, and software related to these models. My application area has been modeling infectious disease outbreaks, but I am also interested in a variety of other topics and application areas.\nMy thesis advisor is Edward Ionides.\nUSU Stat 5100 Lecture Material"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jesse Wheeler",
    "section": "Education",
    "text": "Education\nUniversity of Michigan | Ann Arbor, MI PhD in Statistics | August 2020 - Current\nUtah State University | Logan, UT B.S. in Mathematics and Statistics | August 2016 - May 2020"
  },
  {
    "objectID": "index.html#selected-papers",
    "href": "index.html#selected-papers",
    "title": "Jesse Wheeler",
    "section": "Selected Papers",
    "text": "Selected Papers\n\n\nWheeler, J., et al. 2024. “Informing policy via dynamic models: Cholera in Haiti”. PLOS Computational Biology.\n\n\nWagstaff, J., Bean, B., Wheeler, J., Maguire, M., Sun, Y. 2024. “Adaptive Mapping of Design Ground Snow Loads in the Conterminous United States”. Journal of Structural Engineering.\n\n\nWheeler, J., Ionides, E. L. 2023. “Likelihood Based Inference of ARMA Models”. ArXiv preprint. arXiv.2310.01198.\n\n\nIonides, E. L., Ning, N., Wheeler, J. 2022. “An Iterated Block Particle Filter for Inference on Coupled Dynamic Systems with Shared and Unit-Specific Parameters”. Statistica Sinica. pre-published online.\n\n\nWheeler, J., Bean, B., Maguire, M. 2022. “Creating a universal depth-to-load conversion technique for the conterminous United States using random forests”. Journal of Cold Regions Engineering."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Jesse Wheeler",
    "section": "Software",
    "text": "Software\n\narima2: This library aids maximum likelihood estimation of parameters of ARIMA time series models. The package is currently on CRAN, and there is an associated pre-print paper on ArXiv.\npanelPomp: This R package on CRAN is used for inference of Panel POMP models. I am the current maintainer of this package, and creator and admin for the corresponding GitHub organization.\n\n\nThis website was created using Quarto. To learn more about Quarto websites, see slides I created for a Statistics Student Seminar at the University of Michigan, 2024."
  }
]